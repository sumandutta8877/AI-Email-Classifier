{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Email_Classifier.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kSrIc2Bvvlyd",
        "outputId": "34a73a5f-0dd7-47d8-d83e-b94d468c467a"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive')\n",
        "%cd /gdrive"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /gdrive\n",
            "/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ObKpDIoRvo8E",
        "outputId": "f26d3a0c-8918-4307-e4d2-0c51e83eeed3"
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn import model_selection, naive_bayes, svm\n",
        "from sklearn.metrics import accuracy_score\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import email\n",
        "import re\n",
        "\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "stopwords = stopwords.words('english') "
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VYpF8hzgvpJ6"
      },
      "source": [
        "data = pd.read_csv(\"/gdrive/My Drive/AI Email Classifier/emails.csv\")"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f6gfZIGLvzZH"
      },
      "source": [
        "Extracting Catagories"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sB88qp8Svpa0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "57b9c281-3d51-4629-dea7-22d0af5d5cf9"
      },
      "source": [
        "catagories = []\n",
        "for name in data['file']:\n",
        "  name=name.split('/')\n",
        "  catagories.append(name[1])\n",
        "\n",
        "data['catagory'] = catagories\n",
        "labels = list(data['catagory'].unique())\n",
        "\n",
        "print(data['catagory'].value_counts())"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "all_documents          128103\n",
            "discussion_threads      58609\n",
            "sent                    57653\n",
            "deleted_items           51356\n",
            "inbox                   44859\n",
            "                        ...  \n",
            "bush_administration         1\n",
            "volume_management           1\n",
            "chinesewallspolicy          1\n",
            "fundies                     1\n",
            "sonoco                      1\n",
            "Name: catagory, Length: 1427, dtype: int64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6jKBusPt9elN"
      },
      "source": [
        "selected_catagories =  ['discussion_thread', 'personal', 'meetings', 'logistics', 'calendar', 'archiving', 'california', 'power', 'deal_communication', 'resumes']\n",
        "\n",
        "data = data[data['catagory'].isin(selected_catagories)]"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XJB_oRbkv6Ai"
      },
      "source": [
        "Extracting Message Body and Headers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TxZiPMDhvpcZ",
        "outputId": "eb31d9af-0a8e-4faa-bda6-f5c4583f8f93"
      },
      "source": [
        "#mail body\n",
        "email_body = []\n",
        "\n",
        "for mail in data['message']:\n",
        "  mail = email.message_from_string(mail)  \n",
        "\n",
        "  # getting message body  \n",
        "  message_body = mail.get_payload()\n",
        "  \n",
        "  email_body.append(message_body)\n",
        "\n",
        "data['message_body'] = email_body\n",
        "\n",
        "\n",
        "#getting headers\n",
        "\n",
        "headers = {\"Date\":[], \"Subject\":[], \"X-Folder\":[], \"X-From\":[], \"X-To\":[]}\n",
        "for mail in data['message']:\n",
        "  mail = email.message_from_string(mail)  \n",
        "\n",
        "  #get other headers\n",
        "  for header in headers.keys():\n",
        "    headers[header].append(mail.get(header))\n",
        "\n",
        "for key in headers.keys():\n",
        "  data[key] = headers[key]\n",
        "data['Date'] = pd.to_datetime(data['Date'])\n",
        "\n",
        "print(data.head())"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                      file  ...                                               X-To\n",
            "6345  arnold-j/personal/1.  ...  \"jennifer.medcalf@enron.com\" <jennifer.medcalf...\n",
            "6346  arnold-j/personal/2.  ...                                   Jennifer Medcalf\n",
            "6347  arnold-j/personal/3.  ...                         jennifer.medcalf@enron.com\n",
            "6348  arnold-j/personal/4.  ...                                   Jennifer Medcalf\n",
            "6349  arnold-j/personal/5.  ...                                   Jennifer Medcalf\n",
            "\n",
            "[5 rows x 9 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hhOGVX_3_BT6"
      },
      "source": [
        "Dropping empty rows"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sFUH66Z0_Hdz",
        "outputId": "e49b2946-9bdc-4cf1-91cd-8d1092d4b2d2"
      },
      "source": [
        "print(data.shape)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(19642, 9)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wii2Hva3vphr",
        "outputId": "255297e7-2d23-4cb2-c946-d41bcaa335ab"
      },
      "source": [
        "data.dropna(inplace = True)\n",
        "print(data.shape)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(19623, 9)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qY0dE4Vr466f"
      },
      "source": [
        "Text cleaning\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KMinNNPC45qu"
      },
      "source": [
        "#Removing non alphanumeric characters\n",
        "def clean_text(text):\n",
        "  cleanText = text.lower()\n",
        "  cleanText = re.sub(r'[\\W\\d]', \" \", cleanText)  \n",
        "\n",
        "  return cleanText\n",
        "\n",
        "#Removing stop words\n",
        "def stop_word_removal(text):\n",
        "  tokens = [token for token in text if token not in stopwords]\n",
        "  return tokens\n",
        "\n",
        "#Tokenizing\n",
        "def tokenize(text):\n",
        "  tokens = text.split(\" \")\n",
        "  return tokens\n",
        "\n",
        "headers = [\"Subject\", \"X-Folder\", \"X-From\", \"X-To\", \"message_body\"]\n",
        "\n",
        "tokens = [] \n",
        "\n",
        "for i in range(data.shape[0]):\n",
        "  tokens_i = []\n",
        "  for header in headers:\n",
        "    tokens_h = clean_text(data[header].values[i])\n",
        "    tokens_h = tokenize(tokens_h)\n",
        "    tokens_h = stop_word_removal(tokens_h)\n",
        "\n",
        "    tokens_i.extend(tokens_h)\n",
        "\n",
        "  tokens.append(tokens_i)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JzIdlKhw5egQ",
        "outputId": "0a2a62d9-73fb-45ce-e1ca-ba75eb724eca"
      },
      "source": [
        "for i in range(len(tokens)):\n",
        "  tokens[i].remove(\"\")\n",
        "  tokens[i] = \" \".join(tokens[i])\n",
        "\n",
        "data['final_text'] = tokens\n",
        "\n",
        "print(data['final_text'])"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "6345      dell order confirmation john_arnold_nov     no...\n",
            "6346      brown bag thank john_arnold_nov     notes fold...\n",
            "6347      dell online order john_arnold_nov     notes fo...\n",
            "6348      update  attendees brown bags                jo...\n",
            "6349      confirmation order          john_arnold_nov   ...\n",
            "                                ...                        \n",
            "516848    duke westcoast transaction exmerge   zufferli ...\n",
            "516849    duke westcoast transaction exmerge   zufferli ...\n",
            "516850    updated edcc ecc pricing discussion  exmerge  ...\n",
            "516851    aes project  tolling interest  exmerge   zuffe...\n",
            "516852    transfer enron direct contracts ed  marking in...\n",
            "Name: final_text, Length: 19623, dtype: object\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7W9Dtf8jFQb1"
      },
      "source": [
        "Applying Train-Test split"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nfQtLuUYPGsc"
      },
      "source": [
        "x_train, x_test, y_train, y_test = model_selection.train_test_split(data['final_text'], data['catagory'], test_size=0.3)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PapDI_TSFXY2"
      },
      "source": [
        "Encoding data catagories"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qd0dcCUDg7Du"
      },
      "source": [
        "Encoder = LabelEncoder()\n",
        "y_train = Encoder.fit_transform(y_train)\n",
        "y_test = Encoder.fit_transform(y_test)"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "02uGX_ltFgKD"
      },
      "source": [
        "Applying TF-IDF vectorizer on final text"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hO8AFcY6e49K"
      },
      "source": [
        "Tfidf_vect = TfidfVectorizer()\n",
        "\n",
        "Tfidf_vect.fit(data['final_text'])\n",
        "\n",
        "x_train = Tfidf_vect.transform(x_train)\n",
        "x_test = Tfidf_vect.transform(x_test)"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i6xPVR5vGEvD"
      },
      "source": [
        "Applying Naive Bayes model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BK9kZNIZvpjR",
        "outputId": "23575171-89bd-43e7-e45b-f1ac51c02a52"
      },
      "source": [
        "Naive = naive_bayes.MultinomialNB()\n",
        "Naive.fit(x_train, y_train)\n",
        "\n",
        "predictions_NB = Naive.predict(x_test)\n",
        "print(\"Naive Bayes Accuracy Score -> \", accuracy_score(predictions_NB, y_test)*100)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Naive Bayes Accuracy Score ->  58.077119075930014\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sT-YJ8KAGLWc"
      },
      "source": [
        "Applying SVM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v9-XsITij1Kb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "89821022-3d63-4ac7-f407-e06dba60e718"
      },
      "source": [
        "SVM = svm.SVC(C=1.0, kernel='linear', degree=3, gamma='auto')\n",
        "SVM.fit(x_train, y_train)\n",
        "\n",
        "predictions_SVM = SVM.predict(x_test)\n",
        "print(\"SVM Accuracy Score -> \",accuracy_score(predictions_SVM, y_test)*100)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "SVM Accuracy Score ->  87.42993035501954\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KYf1oesFGX8w"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}